{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "128d1d09-3011-43f5-8e5c-6132f3fea03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On charge les pakcages pour tester enn interne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b829fd23-fd43-4834-a31f-bc877ce06510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "\n",
    "# warnings.filterwarnings(action='always')\n",
    "# warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "# warnings.filterwarnings(\"ignore\",category=ResourceWarning)\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50807725-da51-4442-8ea3-96b993e38cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keyboard in c:\\users\\evari\\anaconda3\\lib\\site-packages (0.13.5)\n",
      "Requirement already satisfied: bs4 in c:\\users\\evari\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\evari\\anaconda3\\lib\\site-packages (from bs4) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\evari\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.2.1)\n",
      "Requirement already satisfied: autocorrect in c:\\users\\evari\\anaconda3\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied: bs4 in c:\\users\\evari\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\evari\\anaconda3\\lib\\site-packages (from bs4) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\evari\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.2.1)\n",
      "Requirement already satisfied: autocorrect in c:\\users\\evari\\anaconda3\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied: wordcloud in c:\\users\\evari\\anaconda3\\lib\\site-packages (1.8.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\evari\\anaconda3\\lib\\site-packages (from wordcloud) (3.3.4)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\evari\\appdata\\roaming\\python\\python38\\site-packages (from wordcloud) (1.19.5)\n",
      "Requirement already satisfied: pillow in c:\\users\\evari\\anaconda3\\lib\\site-packages (from wordcloud) (8.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\evari\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\evari\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\evari\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\evari\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: six in c:\\users\\evari\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.15.0)\n",
      "Requirement already satisfied: yellowbrick in c:\\users\\evari\\appdata\\roaming\\python\\python38\\site-packages (1.3.post1)\n",
      "Requirement already satisfied: cycler>=0.10.0 in c:\\users\\evari\\anaconda3\\lib\\site-packages (from yellowbrick) (0.10.0)\n",
      "Requirement already satisfied: numpy<1.20,>=1.16.0 in c:\\users\\evari\\appdata\\roaming\\python\\python38\\site-packages (from yellowbrick) (1.19.5)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.2 in c:\\users\\evari\\anaconda3\\lib\\site-packages (from yellowbrick) (3.3.4)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\evari\\anaconda3\\lib\\site-packages (from yellowbrick) (1.6.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in c:\\users\\evari\\anaconda3\\lib\\site-packages (from yellowbrick) (0.24.1)\n",
      "Requirement already satisfied: six in c:\\users\\evari\\anaconda3\\lib\\site-packages (from cycler>=0.10.0->yellowbrick) (1.15.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\evari\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\evari\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (8.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\evari\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\evari\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (2.8.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\evari\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20->yellowbrick) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\evari\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20->yellowbrick) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "# Importation des packages souhaités\n",
    "%run P5_Packages.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7b036b4-0572-40bc-a6c8-9b5fcfa5542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = Speller()\n",
    "token = ToktokTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "charac = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~0123456789'\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# List of Adjective's tag from nltk package\n",
    "adjective_tag_list = set(['JJ','JJR', 'JJS', 'RBR', 'RBS']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d76087b0-ccbb-4151-8971-ac82ed4a4fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5d711f0-e1d4-4f9e-b802-f6a1c23b18fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some important text'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text()\n",
    "    return stripped_text\n",
    "\n",
    "strip_html_tags('<html><h2>Some important text</h2></html>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72b9e153-9630-4b1b-b063-67514d3be9cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some Accented text , mm, 65, a %%$'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "remove_accented_chars('Sómě Áccěntěd těxt , mm, 65, à %%££$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26bc9c3d-8314-4bf6-9dc9-08e0efb96b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You all cannot expand contractions I would think, mm, 65, à %%££$'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "expand_contractions(\"Y'all can't expand contractions I'd think, mm, 65, à %%££$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e685914-6185-4bba-999b-6652ffa25eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This [[  P pp  text line is \\\\ gonna get weirdo  \\\\[\\\\]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    # Modification pour les carac qui ne passent pas\n",
    "    text = text.replace('^', '!')\n",
    "    text = text.replace('_', '!')\n",
    "    text = text.replace('`', '!')\n",
    "    \n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "remove_special_characters(\"This [[ ! P pp ^^ text line is \\\\// gonna get weirdo : #$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~0123456789\", \n",
    "                          remove_digits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54503a8f-1d34-44e2-abd9-66a859ea57bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my system keep crash hi crash yesterday, our crash daily, mm, 65, à %%££$'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simple_stemmer(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "simple_stemmer(\"My system keeps crashing his crashed yesterday, ours crashes daily, mm, 65, à %%££$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a122cf57-deae-4ca5-9ca5-dbd5b36a4315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my system keep crash ! his crash yesterday , ours crash daily , mm , 65 , à % % £ £ $'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "lemmatize_text(\"My system keeps crashing! his crashed yesterday, ours crashes daily, mm, 65, à %%££$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "915a334e-e4ea-4581-9fe0-be9611ed0db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', , stopwords , computer not , mm , 65 , à % % £ £ $'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "remove_stopwords(\"The, and, if are stopwords, computer is not, mm, 65, à %%££$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6e4cb8-5cf8-4d25-aea4-d11f6b2d9b55",
   "metadata": {},
   "source": [
    "# Function regrouping all previous functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f46fe34b-b403-431b-ba58-b34c03be18c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True, \n",
    "                     text_lemmatization=True, special_char_removal=True, \n",
    "                     stopword_removal=True, remove_digits=True):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "        # expand contractions    \n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "        # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "        # remove special characters and\\or digits    \n",
    "        if special_char_removal:\n",
    "            # insert spaces between special characters to isolate them    \n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub('\\\\1', doc)\n",
    "            # Slight modification to get rid of [] and \\\\\n",
    "            doc = doc.replace('\\\\', '!')\n",
    "            doc = doc.replace('[', '!')\n",
    "            doc = doc.replace(']', '!')\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "712c73e1-0154-4541-8765-2db7a5dfa3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_txt = {\"US ? !! unv?eils #$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~0123456789 world's \\\\ most  ] powerful //supercomputer, \\ beats \\ China.The US has unveiled the world's  \\/ most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight. With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, which reportedly take up the size of two tennis courts.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79ac20d2-141e-446b-80f2-e86f43d6eaa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unveil world powerful supercomputer beat chinathe us unveil world powerful supercomputer call summit beat previous record holder china sunway taihulight peak performance trillion calculation per second twice fast sunway taihulight capable trillion calculation per second summit server reportedly take size two tennis court']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_corpus = normalize_corpus(full_txt)\n",
    "new_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a9e6993-2b9a-4f7e-8bcb-9576c2008292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\evari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92768d21-2688-42fa-85b4-0d5823c26594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove specific word like an adjective\n",
    "def remove_by_tag(text, undesired_tag):\n",
    "#---remove all words by using ntk tag (adjectives, verbs, etc.)------------------------------\n",
    "    words = tokenizer.tokenize(text) # Tokenize each words\n",
    "    words_tagged = nltk.pos_tag(tokens=words, tagset=None, lang='eng') # Tag each words and return a list of tuples (e.g. (\"have\", \"VB\"))\n",
    "    filtered = [w[0] for w in words_tagged if w[1] not in undesired_tag] # Select all words that don't have the undesired tags\n",
    "    \n",
    "    return ' '.join(map(str, filtered)) # Return the text untokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1b240fb-8b8b-44f1-8182-97550bc32ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjective_tag_list = set(['JJ','JJR', 'JJS', 'RBR', 'RBS']) \n",
    "# List of Adjective's tag from nltk package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92a02670-c25e-4410-acfd-05366ffb1cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_adj = ' world supercomputer beat us world supercomputer call summit beat record holder china sunway peak performance trillion calculation per twice fast taihulight trillion calculation per summit server reportedly take size two tennis court '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9913e373-fb5b-447b-ab8c-0ca1a640c631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'world supercomputer beat us world supercomputer call summit beat record holder china sunway performance trillion calculation per twice fast trillion calculation per summit server reportedly take size two tennis court'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_txt = remove_by_tag(txt_adj, adjective_tag_list)\n",
    "new_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86d89705-2128-48f3-b715-0cc996465bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_corpus_ADJ(corpus, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True, \n",
    "                     text_lemmatization=True, special_char_removal=True, \n",
    "                     stopword_removal=True, remove_digits=True, remove_adjectives=True):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "        # expand contractions    \n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "        # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "        # remove special characters and\\or digits    \n",
    "        if special_char_removal:\n",
    "            # insert spaces between special characters to isolate them    \n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub('\\\\1', doc)\n",
    "            # Slight modification to get rid of [] and \\\\\n",
    "            doc = doc.replace('\\\\', '!')\n",
    "            doc = doc.replace('[', '!')\n",
    "            doc = doc.replace(']', '!')\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "        # remove adjectives\n",
    "        if remove_adjectives:\n",
    "            doc = remove_by_tag(doc, adjective_tag_list)\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "834035ab-33c4-4bcf-bcae-a545e19aa7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['world supercomputer beat us world supercomputer call summit beat record holder china sunway peak performance trillion calculation per twice fast taihulight trillion calculation per summit server reportedly take size two tennis court']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_corpus = normalize_corpus_ADJ(full_txt)\n",
    "new_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10176a1-bea1-42a1-8f65-4a378e7ec2f1",
   "metadata": {},
   "source": [
    "# Fonction de nettoyage utilisées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4675b5f-2dd1-4551-a95b-a29ee410e4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"\\'\", \"'\", text) # match all literal apostrophe pattern then replace them by a single whitespace\n",
    "    text = re.sub(r\"\\n\", \" \", text) # match all literal Line Feed (New line) pattern then replace them by a single whitespace\n",
    "    text = re.sub(r\"\\xa0\", \" \", text) # match all literal non-breakable space pattern then replace them by a single whitespace\n",
    "    text = re.sub('\\s+', ' ', text) # match all one or more whitespace then replace them by a single whitespace\n",
    "    text = text.strip(' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "587a5c7c-5ab3-4b6a-a400-f992adaf3e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    \"\"\"expand shortened words, e.g. 'don't' to 'do not'\"\"\"\n",
    "    text = contractions.fix(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc57dd0a-5852-4ad9-8718-413150ed837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21c032d9-adf6-4b91-a7d0-89b4f54d3784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrect(text):\n",
    "    words = token.tokenize(text)\n",
    "    \n",
    "    # Option 'fast' in place\n",
    "    spell = Speller(fast=True)\n",
    "    \n",
    "    words_correct = [spell(w) for w in words]\n",
    "    return ' '.join(map(str, words_correct)) # Return the text untokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b879b7c-6c93-4264-b345-7ff97e3e91d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_and_number(text):\n",
    "    \"\"\"remove all punctuation and number\"\"\"\n",
    "    return text.translate(str.maketrans(\" \", \" \", charac)) \n",
    "\n",
    "\n",
    "def remove_non_alphabetical_character(text):\n",
    "    \"\"\"remove all non-alphabetical character\"\"\"\n",
    "    text = re.sub(\"[^a-z]+\", \" \", text) # remove all non-alphabetical character\n",
    "    text = re.sub(\"\\s+\", \" \", text) # remove whitespaces left after the last operation\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "935d6055-5681-4f65-a5e5-ccaa5b30e6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_letter(text):\n",
    "    \"\"\"remove single alphabetical character\"\"\"\n",
    "    text = re.sub(r\"\\b\\w{1}\\b\", \"\", text) # remove all single letter\n",
    "    text = re.sub(\"\\s+\", \" \", text) # remove whitespaces left after the last operation\n",
    "    text = text.strip(\" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b545a4d3-9c49-43b5-b986-521f55cb4827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\"remove common words in english by using nltk.corpus's list\"\"\"\n",
    "    words = token.tokenize(text)\n",
    "    filtered = [w for w in words if not w in stop_words]\n",
    "    \n",
    "    return ' '.join(map(str, filtered)) # Return the text untokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1c08130-c4e6-4ae7-98c5-63765391f768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_by_tag(text, undesired_tag):\n",
    "    \"\"\"remove all words by using ntk tag (adjectives, verbs, etc.)\"\"\"\n",
    "    words = token.tokenize(text) # Tokenize each words\n",
    "    words_tagged = nltk.pos_tag(tokens=words, tagset=None, lang='eng') # Tag each words and return a list of tuples (e.g. (\"have\", \"VB\"))\n",
    "    filtered = [w[0] for w in words_tagged if w[1] not in undesired_tag] # Select all words that don't have the undesired tags\n",
    "    \n",
    "    return ' '.join(map(str, filtered)) # Return the text untokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa70a578-8ff1-4db9-b336-9d59ad9d4af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text(text):\n",
    "    \"\"\"Stem the text\"\"\"\n",
    "    words = nltk.word_tokenize(text) # tokenize the text then return a list of tuple (token, nltk_tag)\n",
    "    stem_text = []\n",
    "    for word in words:\n",
    "        stem_text.append(stemmer.stem(word)) # Stem each words\n",
    "    return \" \".join(stem_text) # Return the text untokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3de711cf-af8b-420e-9ff0-34579af0e9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    \"\"\"Lemmatize the text by using tag \"\"\"\n",
    "    \n",
    "    tokens_tagged = nltk.pos_tag(nltk.word_tokenize(text))  # tokenize the text then return a list of tuple (token, nltk_tag)\n",
    "    lemmatized_text = []\n",
    "    for word, tag in tokens_tagged:\n",
    "        if tag.startswith('J'):\n",
    "            lemmatized_text.append(lemmatizer.lemmatize(word,'a')) # Lemmatisze adjectives. Not doing anything since we remove all adjective\n",
    "        elif tag.startswith('V'):\n",
    "            lemmatized_text.append(lemmatizer.lemmatize(word,'v')) # Lemmatisze verbs\n",
    "        elif tag.startswith('N'):\n",
    "            lemmatized_text.append(lemmatizer.lemmatize(word,'n')) # Lemmatisze nouns\n",
    "        elif tag.startswith('R'):\n",
    "            lemmatized_text.append(lemmatizer.lemmatize(word,'r')) # Lemmatisze adverbs\n",
    "        else:\n",
    "            lemmatized_text.append(lemmatizer.lemmatize(word)) # If no tags has been found, perform a non specific lemmatization\n",
    "    return \" \".join(lemmatized_text) # Return the text untokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ebceba-4b9d-444a-a4dd-e98ae4896ece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2683eb73-0859-41dc-ab27-74e736f952dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
